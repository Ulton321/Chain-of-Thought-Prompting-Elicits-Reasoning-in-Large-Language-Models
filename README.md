# Chain-of-Thought-Prompting-Elicits-Reasoning-in-Large-Language-Models

This paper discusses the drawbacks of Chain-of-Thought (CoT) prompting, claiming that CoT does not teach Large Language Models (LLMs) base algorithmic procedures as claimed. Instead, the prompt is usually carefully engineered and very specific to the problem to be solved at hand, not being easily generalized to many problems. This paper also highlights the drawback of the amount of human labor necessary to generate such examples for CoT prompting, which effectively reduces the advantages such as possible performance gains even further, given the fact that these CoT prompting techniques also do not scale well. The study evaluates CoT prompting across different levels of problem complexity and prompt generality using models such as GPT-4 and Claude-3-Opus, showing that performance gains are only seen with highly specific prompts and quickly deteriorate as problems become more complex or generalized. Also, usually CoT works on a narrow "sweet spot" where the problem is not so easy that this technique does not help much, or so hard that even CoT does not help solve the problem, narrowing down the utility of this method a lot. The main conclusion of this paper is that it effectively disarms the argument that CoT teaches "reasoning skills" to LLMs, and makes that argument much weaker.

Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting-a method of demonstrating solution procedures-with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem. This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples. We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes. Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.

